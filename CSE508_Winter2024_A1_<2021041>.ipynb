{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6W0N2ITLXeZ"
      },
      "source": [
        "Q1. **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7v7-i8eITHF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orDzcYCX5CBd"
      },
      "outputs": [],
      "source": [
        "pip install beautifulsoup4 nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSFk25H_5HMq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqNpx7WM6ZyG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to perform data preprocessing\n",
        "def preprocess_text(input_text):\n",
        "\n",
        "    # a. Convert text to lowercase\n",
        "    lowercase_text = input_text.lower()\n",
        "\n",
        "    # b. Tokenization using NLTK\n",
        "    tokens = word_tokenize(lowercase_text)\n",
        "\n",
        "    # c. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # d. Remove punctuation\n",
        "    no_punctuations = [word for word in filtered_tokens if word.isalnum()]\n",
        "\n",
        "    # e. Remove blank space tokens\n",
        "    no_blank_space = [word for word in no_punctuations if word.strip() != '']\n",
        "\n",
        "    # Combine the tokens into a preprocessed text\n",
        "    preprocessed_text = ' '.join(no_blank_space)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "def preprocessed_folder(folder_path, output_directory):\n",
        "\n",
        "    for count, filename in enumerate(os.listdir(folder_path), start=1):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            output_path = os.path.join(output_directory, f'processed_{filename}')\n",
        "\n",
        "            with open(file_path, 'r') as file, open(output_path, 'w') as output_file:\n",
        "                text = file.read()\n",
        "                preprocessed_text = preprocess_text(text)\n",
        "                output_file.write(preprocessed_text)\n",
        "\n",
        "            # Print contents for the first 5 files\n",
        "            if count <= 5:\n",
        "                print(f\"Original Content of {filename} (File {count}):\\n{text}\\n\")\n",
        "                print(f\"Preprocessed Content of {filename} (File {count}):\\n{preprocessed_text}\\n\")\n",
        "\n",
        "\n",
        "# Folder containing text files\n",
        "folder_path = '/content/drive/MyDrive/IR_Assignments/text_files_IR'\n",
        "\n",
        "# Preprocessed files are saved here\n",
        "output_directory = '/content/drive/MyDrive/IR_Assignments/output_directory_IR_1'\n",
        "\n",
        "# Preprocessed folder and prints its contents\n",
        "preprocessed_folder(folder_path, output_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEBO6VsYLG2l"
      },
      "source": [
        "Q2. **Unigram Inverted Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxAf3SY-LEtZ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Function to create a unigram inverted index\n",
        "def create_inverted_index(dataset_path):\n",
        "    inverted_index = {}\n",
        "\n",
        "    # Iterate through each preprocessed file in the dataset\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        if filename.startswith('processed_') and filename.endswith('.txt'):\n",
        "            file_path = os.path.join(dataset_path, filename)\n",
        "\n",
        "            with open(file_path, 'r') as file:\n",
        "                text = file.read()\n",
        "                tokens = word_tokenize(text)\n",
        "\n",
        "                # Removing duplicate tokens\n",
        "                unique_tokens = list(set(tokens))\n",
        "\n",
        "                # Updating the inverted index for each term in the document\n",
        "                for i in unique_tokens:\n",
        "\n",
        "                  if i not in inverted_index:\n",
        "                      inverted_index[i] = [filename]\n",
        "                  else:\n",
        "                      inverted_index[i].append(filename)\n",
        "\n",
        "    return inverted_index\n",
        "\n",
        "# Specify the path to your preprocessed files directory\n",
        "preprocessed_files_directory = '/content/drive/MyDrive/IR_Assignments/output_directory_IR_1'\n",
        "\n",
        "# Create the unigram inverted index\n",
        "unigram_inverted_index = create_inverted_index(preprocessed_files_directory)\n",
        "\n",
        "# Printing the unigram inverted index\n",
        "# for term, documents in unigram_inverted_index.items():\n",
        "#    print(f\"{term}: {documents}\")\n",
        "\n",
        "# print('\\n')\n",
        "\n",
        "# Save the unigram inverted index using pickle\n",
        "output_index_path = '/content/drive/MyDrive/IR_Assignments/unigram_inverted_index/unigram_inverted_index.pkl'\n",
        "with open(output_index_path, 'wb') as output_file:\n",
        "    pickle.dump(unigram_inverted_index, output_file)\n",
        "\n",
        "print(f\"Unigram inverted index saved to {output_index_path}\")\n",
        "\n",
        "# Load the unigram inverted index using pickle\n",
        "input_index_path = '/content/drive/MyDrive/IR_Assignments/unigram_inverted_index/unigram_inverted_index.pkl'\n",
        "with open(input_index_path, 'rb') as input_file:\n",
        "    loaded_unigram_inverted_index = pickle.load(input_file)\n",
        "\n",
        "# Now, loaded_unigram_inverted_index contains the previously saved index and then printing it\n",
        "print(f\"Unigram inverted index loaded from {input_index_path}\")\n",
        "print(loaded_unigram_inverted_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkuacaBJpPtS"
      },
      "source": [
        "**Boolean Queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80nuZCeCE3eP",
        "outputId": "ad7e8217-4eb3-4d49-b93e-8f08cc5f1f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 2\n",
            "Enter query 1: Car in a Canister\n",
            "Enter operations: OR\n",
            "Enter query 2: Fits can CAR in a Canister\n",
            "Enter operations: OR, OR\n",
            "Query 1: car OR canister\n",
            "Number of documents retrieved: 6\n",
            "Documents: processed_file166.txt, processed_file174.txt, processed_file264.txt, processed_file542.txt, processed_file746.txt, processed_file886.txt\n",
            "Query 2: fits OR car OR canister\n",
            "Number of documents retrieved: 66\n",
            "Documents: processed_file107.txt, processed_file110.txt, processed_file118.txt, processed_file166.txt, processed_file169.txt, processed_file174.txt, processed_file176.txt, processed_file179.txt, processed_file187.txt, processed_file189.txt, processed_file221.txt, processed_file242.txt, processed_file264.txt, processed_file285.txt, processed_file302.txt, processed_file324.txt, processed_file341.txt, processed_file346.txt, processed_file364.txt, processed_file38.txt, processed_file39.txt, processed_file431.txt, processed_file454.txt, processed_file460.txt, processed_file466.txt, processed_file500.txt, processed_file508.txt, processed_file511.txt, processed_file517.txt, processed_file531.txt, processed_file537.txt, processed_file542.txt, processed_file549.txt, processed_file559.txt, processed_file574.txt, processed_file581.txt, processed_file61.txt, processed_file616.txt, processed_file626.txt, processed_file63.txt, processed_file639.txt, processed_file648.txt, processed_file719.txt, processed_file73.txt, processed_file733.txt, processed_file746.txt, processed_file756.txt, processed_file760.txt, processed_file804.txt, processed_file806.txt, processed_file815.txt, processed_file838.txt, processed_file862.txt, processed_file864.txt, processed_file886.txt, processed_file891.txt, processed_file90.txt, processed_file915.txt, processed_file930.txt, processed_file934.txt, processed_file942.txt, processed_file956.txt, processed_file976.txt, processed_file981.txt, processed_file986.txt, processed_file994.txt\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import string\n",
        "\n",
        "# stopwords - nltk\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def load_inverted_index(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "\n",
        "def parse_operations(operations_str):\n",
        "    return operations_str.split(', ')\n",
        "\n",
        "def execute_query(terms, operations, inverted_index):\n",
        "    result_docs = set(inverted_index.get(terms[0], []))\n",
        "    for i, term in enumerate(terms[1:], start=1):\n",
        "        current_docs = set(inverted_index.get(term, []))\n",
        "        operation = operations[i-1] if i-1 < len(operations) else None\n",
        "\n",
        "        if operation == 'AND':\n",
        "            result_docs &= current_docs\n",
        "        elif operation == 'OR':\n",
        "            result_docs |= current_docs\n",
        "        elif operation == 'AND NOT':\n",
        "            result_docs -= current_docs\n",
        "        elif operation == 'OR NOT':\n",
        "          #dont understand how it works\n",
        "            continue\n",
        "\n",
        "    return sorted(result_docs)\n",
        "\n",
        "def format_output(query_number, terms, operations, results):\n",
        "    query_str = ' '.join([f\"{terms[i]} {operations[i]}\" if i < len(operations) else terms[i] for i in range(len(terms))])\n",
        "    result_str = ', '.join(results) if results else \"No documents found\"\n",
        "    print(f\"Query {query_number}: {query_str}\")\n",
        "    print(f\"Number of documents retrieved: {len(results)}\")\n",
        "    print(f\"Documents: {result_str}\")\n",
        "\n",
        "def main():\n",
        "    inverted_index = load_inverted_index('/content/drive/MyDrive/IR_Assignments/unigram_inverted_index/unigram_inverted_index.pkl')\n",
        "    num_queries = int(input(\"Enter the number of queries: \"))\n",
        "    queries = []\n",
        "    operations_list = []\n",
        "\n",
        "    # Collect queries and operations\n",
        "    for i in range(num_queries):\n",
        "        query = input(f\"Enter query {i+1}: \")\n",
        "        operations_input = input(\"Enter operations: \")\n",
        "        queries.append((query, operations_input))\n",
        "\n",
        "    # Process each query and store results\n",
        "    results = []\n",
        "    for i, (query, operations_input) in enumerate(queries, start=1):\n",
        "        terms = preprocess(query)\n",
        "        operations = parse_operations(operations_input)\n",
        "\n",
        "        if len(operations) != len(terms) - 1:\n",
        "            results.append(f\"Error: Number of operations should be one less than the number of terms for Query {i}\")\n",
        "            continue\n",
        "\n",
        "        result_docs = execute_query(terms, operations, inverted_index)\n",
        "        results.append(format_output(i, terms, operations, result_docs))\n",
        "\n",
        "    # Print all results together\n",
        "    for result in results:\n",
        "      if(result):\n",
        "        print(result)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUnaXHf0pljd"
      },
      "source": [
        "Q3. **Positional Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwDTjSPaZ6pS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "\n",
        "def create_positional_index(docs_folder):\n",
        "    positional_index = {}\n",
        "\n",
        "    # Loop through each document in the dataset\n",
        "    for filename in os.listdir(docs_folder):\n",
        "        file_path = os.path.join(docs_folder, filename)\n",
        "\n",
        "        # Read the content of the document\n",
        "        with open(file_path, 'r') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Preprocess the text and get the terms\n",
        "        terms = preprocess(text)\n",
        "\n",
        "        # Update the positional index with terms from this document\n",
        "        for position, term in enumerate(terms):\n",
        "            if term not in positional_index:\n",
        "                positional_index[term] = {}\n",
        "            if filename not in positional_index[term]:\n",
        "                positional_index[term][filename] = []\n",
        "            positional_index[term][filename].append(position)\n",
        "\n",
        "    return (positional_index)\n",
        "\n",
        "# folder containing the preprocessed files\n",
        "preprocessed_files_folder = '/content/drive/MyDrive/IR_Assignments/output_directory_IR_1'\n",
        "positional_index = create_positional_index(preprocessed_files_folder)\n",
        "\n",
        "def save_positional_index(positional_index, file_path):\n",
        "    with open(file_path, 'wb') as file:\n",
        "        pickle.dump(positional_index, file)\n",
        "\n",
        "# Save the positional index to a file\n",
        "save_positional_index(positional_index, '/content/drive/MyDrive/IR_Assignments/positional_index/positional_index.pkl')\n",
        "\n",
        "def load_positional_index(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        positional_index = pickle.load(file)\n",
        "    return positional_index\n",
        "\n",
        "# Load the positional index from a file\n",
        "positional_index = load_positional_index('/content/drive/MyDrive/IR_Assignments/positional_index/positional_index.pkl')\n",
        "print(positional_index)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjU_AoV4pzyp"
      },
      "source": [
        "**Phrase Queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qA6U_bk6iL7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f4dbb3-838c-4619-df9e-fd499d0f3ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 1\n",
            "Enter phrase query 1: it is good\n",
            "Number of documents retrieved for query 1 using positional index: 204\n",
            "Names of documents retrieved for query 1 using positional index: processed_file2.txt, processed_file539.txt, processed_file541.txt, processed_file748.txt, processed_file731.txt, processed_file160.txt, processed_file655.txt, processed_file984.txt, processed_file532.txt, processed_file715.txt, processed_file103.txt, processed_file713.txt, processed_file28.txt, processed_file795.txt, processed_file118.txt, processed_file9.txt, processed_file137.txt, processed_file586.txt, processed_file777.txt, processed_file610.txt, processed_file974.txt, processed_file8.txt, processed_file497.txt, processed_file918.txt, processed_file240.txt, processed_file325.txt, processed_file885.txt, processed_file896.txt, processed_file282.txt, processed_file794.txt, processed_file111.txt, processed_file44.txt, processed_file772.txt, processed_file919.txt, processed_file844.txt, processed_file367.txt, processed_file816.txt, processed_file911.txt, processed_file358.txt, processed_file680.txt, processed_file571.txt, processed_file18.txt, processed_file722.txt, processed_file625.txt, processed_file143.txt, processed_file16.txt, processed_file407.txt, processed_file526.txt, processed_file30.txt, processed_file390.txt, processed_file857.txt, processed_file626.txt, processed_file204.txt, processed_file931.txt, processed_file172.txt, processed_file983.txt, processed_file513.txt, processed_file976.txt, processed_file827.txt, processed_file40.txt, processed_file501.txt, processed_file850.txt, processed_file765.txt, processed_file907.txt, processed_file106.txt, processed_file736.txt, processed_file234.txt, processed_file210.txt, processed_file85.txt, processed_file595.txt, processed_file354.txt, processed_file800.txt, processed_file674.txt, processed_file775.txt, processed_file355.txt, processed_file404.txt, processed_file544.txt, processed_file912.txt, processed_file174.txt, processed_file115.txt, processed_file493.txt, processed_file703.txt, processed_file299.txt, processed_file941.txt, processed_file908.txt, processed_file619.txt, processed_file179.txt, processed_file96.txt, processed_file716.txt, processed_file913.txt, processed_file189.txt, processed_file925.txt, processed_file19.txt, processed_file288.txt, processed_file815.txt, processed_file4.txt, processed_file305.txt, processed_file347.txt, processed_file628.txt, processed_file597.txt, processed_file793.txt, processed_file207.txt, processed_file422.txt, processed_file342.txt, processed_file551.txt, processed_file841.txt, processed_file252.txt, processed_file720.txt, processed_file536.txt, processed_file711.txt, processed_file624.txt, processed_file293.txt, processed_file549.txt, processed_file58.txt, processed_file786.txt, processed_file217.txt, processed_file162.txt, processed_file159.txt, processed_file843.txt, processed_file864.txt, processed_file155.txt, processed_file332.txt, processed_file163.txt, processed_file455.txt, processed_file634.txt, processed_file991.txt, processed_file860.txt, processed_file316.txt, processed_file72.txt, processed_file503.txt, processed_file338.txt, processed_file782.txt, processed_file739.txt, processed_file890.txt, processed_file1.txt, processed_file819.txt, processed_file645.txt, processed_file154.txt, processed_file924.txt, processed_file393.txt, processed_file157.txt, processed_file689.txt, processed_file413.txt, processed_file362.txt, processed_file164.txt, processed_file821.txt, processed_file575.txt, processed_file704.txt, processed_file90.txt, processed_file678.txt, processed_file637.txt, processed_file676.txt, processed_file944.txt, processed_file585.txt, processed_file938.txt, processed_file254.txt, processed_file37.txt, processed_file175.txt, processed_file942.txt, processed_file65.txt, processed_file220.txt, processed_file13.txt, processed_file176.txt, processed_file584.txt, processed_file378.txt, processed_file859.txt, processed_file755.txt, processed_file888.txt, processed_file265.txt, processed_file471.txt, processed_file274.txt, processed_file870.txt, processed_file660.txt, processed_file235.txt, processed_file598.txt, processed_file769.txt, processed_file311.txt, processed_file807.txt, processed_file110.txt, processed_file813.txt, processed_file603.txt, processed_file522.txt, processed_file78.txt, processed_file883.txt, processed_file681.txt, processed_file292.txt, processed_file396.txt, processed_file277.txt, processed_file141.txt, processed_file770.txt, processed_file400.txt, processed_file46.txt, processed_file321.txt, processed_file867.txt, processed_file499.txt, processed_file304.txt, processed_file43.txt, processed_file525.txt, processed_file166.txt, processed_file77.txt, processed_file917.txt, processed_file656.txt, processed_file29.txt, processed_file245.txt\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "\n",
        "# Function to load the positional index\n",
        "def load_positional_index(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        positional_index = pickle.load(file)\n",
        "    return positional_index\n",
        "\n",
        "\n",
        "def execute_phrase_query(query, positional_index):\n",
        "    # Preprocess the query\n",
        "    terms = preprocess(query)\n",
        "\n",
        "    # Check if the first term is in the positional index\n",
        "    if terms[0] not in positional_index:\n",
        "        return set()\n",
        "\n",
        "    # Initialize potential result documents with the first term's documents\n",
        "    potential_docs = positional_index[terms[0]].keys()\n",
        "\n",
        "    # This will hold the final document IDs where the phrase is found\n",
        "    final_docs = set()\n",
        "\n",
        "    for doc in potential_docs:\n",
        "        # Lists to store term positions for each term in the query\n",
        "        term_positions = [positional_index[term][doc] for term in terms if term in positional_index and doc in positional_index[term]]\n",
        "\n",
        "        # Skip documents that don't have all terms\n",
        "        if len(term_positions) != len(terms):\n",
        "            continue\n",
        "\n",
        "        # Check for consecutive positions\n",
        "        # Iterate over each position of the first term\n",
        "        for start_pos in term_positions[0]:\n",
        "            # Check if for each subsequent term there's a position = previous position + 1\n",
        "            if all((start_pos + i in positions) for i, positions in enumerate(term_positions)):\n",
        "                final_docs.add(doc)\n",
        "                break  # No need to check other start positions if one sequence matches\n",
        "\n",
        "    return final_docs\n",
        "\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the positional index\n",
        "    positional_index_path = '/content/drive/MyDrive/IR_Assignments/positional_index/positional_index.pkl'\n",
        "    positional_index = load_positional_index(positional_index_path)\n",
        "\n",
        "    num_queries = int(input(\"Enter the number of queries: \"))\n",
        "\n",
        "    # Lists to store queries and results\n",
        "    queries = []\n",
        "    all_results = []\n",
        "\n",
        "    # Collect all queries first\n",
        "    for i in range(num_queries):\n",
        "        query_input = input(f\"Enter phrase query {i+1}: \")\n",
        "        queries.append(query_input)\n",
        "\n",
        "    # Process each query\n",
        "    for query in queries:\n",
        "        results = execute_phrase_query(query, positional_index)\n",
        "        all_results.append((query, results))\n",
        "\n",
        "    # Display all results together\n",
        "    for i, (query, results) in enumerate(all_results):\n",
        "    # Print the query number and the number of documents retrieved\n",
        "      print(f\"Number of documents retrieved for query {i+1} using positional index: {len(results)}\")\n",
        "\n",
        "      # Check if there are results\n",
        "      if results:\n",
        "          # If there are results, join them into a string and print\n",
        "          documents_str = ', '.join(results)\n",
        "          print(f\"Names of documents retrieved for query {i+1} using positional index: {documents_str}\")\n",
        "      else:\n",
        "          print(f\"Names of documents retrieved for query {i+1} using positional index: No documents found\")\n",
        "\n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}